# Learning Not to Learn: Training Deep Neural Networks with Biased Data

## Abstract
We propose a novel regularization algorithm to train
deep neural networks, in which data at training time is
severely biased. Since a neural network efficiently learns
data distribution, a network is likely to learn the bias information
to categorize input data. It leads to poor performance
at test time, if the bias is, in fact, irrelevant to the
categorization. In this paper, we formulate a regularization
loss based on mutual information between feature embedding
and bias. Based on the idea of minimizing this mutual
information, we propose an iterative algorithm to unlearn
the bias information. We employ an additional network to
predict the bias distribution and train the network adversarially
against the feature embedding network. At the end of
learning, the bias prediction network is not able to predict
the bias not because it is poorly trained, but because the
feature embedding network successfully unlearns the bias
information. We also demonstrate quantitative and qualitative
experimental results which show that our algorithm
effectively removes the bias information from feature embedding.

## Summary in russian
Авторы обучают сеть на данных содержащих биас так, чтобы она робастно работала на данных без биаса. Предполагается, что биас при этом заранее известен и для него есть отдельный лейбл.

Подход демонстрируется на трех датасетах:
* раскрашенный MNIST, где каждой цифре соответствует свой цвет в train-части, и рандомно назначен в test-части;
* Dogs and Cats, где для одного класса выбраны животные темного окраса, а для второго светлого;
* IMDB-Face: здесь лейблами выступают пол и возраст для лиц с imdb.com. В обучающей части, данные опять же разделены так, что одному из полов соотвествует возраст 0-29, а второму 40+.

Решение вдохновлено InfoGAN и выглядит так. Вся сеть делится на три части:
* f(x) -- экстрактор фич;
* g -- классификатор для основной задачи;
* h -- классификатор для биаса, моделирующий P(bias(x)|f(x)).
