# Group normalization
Yuxin Wu, Kaiming He

https://arxiv.org/abs/1803.08494

## Проблема
BN работает хорошо, и без него что-то вообще не тренируется, но это если батч большой. А если батч мал, то быстро растёт ошибка. В задачах сегментации изображений, или в задачах с видео, с большим батчем начинаются проблемы. И если хочешь туда сделать transfer-learning, то батчнорм нужно менять на что-то.

BN:
* нормализует фичи по mean и var из батча
* помогает сходимости
* из-за нестабильности батчей добавляет регуляризации (и генерализации)
* входит в многие SOTA-алгоритмы

Но, требуется достатчоно большой батч (32 per worker). Если меньше, то classification-error начинает расти очень быстро. Но с BN всё сходится лучше, поэтому многие SOTA-модели тренируют с достаточно большим батчем. А в Fast/er и Mask R-CNN используют батч 1 или 2, но замораживают BN. В видео если юзать BN приходится уменьшать temporal-сайз или spatial-сайз.

## Возможные решения
* Batch Renormalization (Ioffe). Добавить два доп. параметра и с их помощью ограничить mean&std, чтобы они сильно не улетали на маленьком батче.
* LayerNorm (along channel) и InstanceNorm (for each instance) + WeightNorm работают норм для RNN и GAN, но хуже BN именно для visual-задач.
* Аккумулировать сэмплы (собирать статистики с нескольких gpu).

## Примеры group-фич
SIFT HOG GIST
VLAD, Fisher Vectors

## Сам метод
Нормализация для фичи $x_i$ выглядит так $\hat{x_i} = \frac1\sigma(x_i - \mu_i)$, где $\mu_i$ и $\simga_i$ это среднее и отклонение, рассчитанные по некоторому множеству. Статистики соответственно получаются общими для этого множества и нормализация делается одинаково.

Для BatcnNorm общее множество это активации на данном канале для всех изображений в батче (N,H,W,C)->(1,1,1,C).
Для LayerNorm общее множество это активации по данному изображению для всех каналов (N,H,W,C)->(N,1,1,1).
Для InstanceNorm общее множество это активации по данному каналу в данному изображении (N,H,W,C)->(N,1,1,C).
Для GroupNorm общее множество это активации группы каналов в данном изображении (N,H,W,C)->(N,1,1,G).
(смотри иллюстрацию с кубиками в статье)

В конце ещё обычно, чтобы не терять выразительную способность фич, добавляют линейное преобразование $y = \gamma\hat{x}+\beta$.

## Результаты
Понятно, что данный метод нормализации не зависит от размера батча, поэтому, в отличии от BN показывает стабильные результаты при разных batchsize. Получается похуже, чем BN с большим батчем, но не на много. Возможно, это связано с тем, что BN имеет регуляризирующий эффект из-за стохатистического рассчета статитстик в трейн-режиме. При этом, способность помогать сходимости сохраняется и train-лосс даже ниже у GN в некоторых экспериментах. А в эксперименте с VGG ниже и validation-лосс (скорее из-за того, что VGG не нуждается в такой сильной регуляризации).
